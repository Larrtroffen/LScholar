import OpenAI from 'openai';
import { configService } from './ConfigService';
import { modelService } from './ModelService';
import { FunctionType } from '../../shared/types';

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface ChatOptions {
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
}

// è¿‡æ»¤æ€è€ƒé“¾å— - åªç§»é™¤æ ‡ç­¾ï¼Œä¸è§¦ç¢°ä»£ç å†…å®¹
function cleanThinkingBlocks(text: string): string {
  // ç§»é™¤ XML æ ¼å¼çš„æ€è€ƒæ ‡ç­¾
  text = text.replace(/<think[^>]*>[\s\S]*?<\/think>/gi, '');
  text = text.replace(/<thinking[^>]*>[\s\S]*?<\/thinking>/gi, '');
  text = text.replace(/<thought[^>]*>[\s\S]*?<\/thought>/gi, '');
  
  // ç§»é™¤ Markdown æ ¼å¼çš„æ€è€ƒæ ‡ç­¾
  text = text.replace(/<thought_block>[\s\S]*?<\/thought_block>/gi, '');
  
  // ç§»é™¤ä»¥ç‰¹å®šå‰ç¼€å¼€å¤´çš„è¡Œï¼ˆè¿™äº›æ˜¯å¸¸è§çš„æ€è€ƒ/åˆ†æå‰ç¼€ï¼‰
  text = text.replace(/^[\s]*[ğŸ“ğŸ’¡ğŸ”ğŸ¤”åˆ†ææ€è€ƒ]?\s*(Thinking|Analysis|æ€è€ƒ|åˆ†æ)[:ï¼š\s].*$/gmi, '');
  
  return text.trim();
}

export class LLMService {
  private getClient(functionType: FunctionType): { client: OpenAI; modelName: string } {
    // æ–‡æœ¬ç”ŸæˆåŠŸèƒ½ç»Ÿä¸€ä½¿ç”¨ main_chat æ¨¡å‹
    if (functionType === 'script_generation' || functionType === 'insight') {
      functionType = 'main_chat';
    }

    const assignment = configService.getAssignment(functionType);
    if (!assignment || assignment.model_id === null) {
      throw new Error(`No model assigned for function: ${functionType}`);
    }

    const model = modelService.getById(assignment.model_id);
    if (!model) {
      throw new Error(`Assigned model not found: ${assignment.model_id}`);
    }

    const client = new OpenAI({
      baseURL: model.base_url,
      apiKey: model.api_key || 'dummy',
      dangerouslyAllowBrowser: true
    });

    return { client, modelName: model.model_name };
  }

  async chat(functionType: FunctionType, messages: ChatMessage[], options: ChatOptions = {}): Promise<string> {
    const { client, modelName } = this.getClient(functionType);

    try {
      const response = await client.chat.completions.create({
        model: modelName,
        messages: messages,
        temperature: options.temperature,
        max_tokens: options.max_tokens,
        stream: false
      });

      const content = response.choices[0]?.message?.content || '';
      return cleanThinkingBlocks(content);
    } catch (error) {
      console.error(`LLM chat error (${functionType}):`, error);
      throw error;
    }
  }

  async *streamChat(functionType: FunctionType, messages: ChatMessage[], options: ChatOptions = {}): AsyncGenerator<string, void, unknown> {
    const { client, modelName } = this.getClient(functionType);

    try {
      const stream = await client.chat.completions.create({
        model: modelName,
        messages: messages,
        temperature: options.temperature,
        max_tokens: options.max_tokens,
        stream: true
      });

      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || '';
        // æµå¼è¾“å‡ºæ—¶ç›´æ¥è¿‡æ»¤æ€è€ƒå—
        const cleaned = cleanThinkingBlocks(content);
        if (cleaned) {
          yield cleaned;
        }
      }
    } catch (error) {
      console.error(`LLM stream chat error (${functionType}):`, error);
      throw error;
    }
  }
}

export const llmService = new LLMService();
